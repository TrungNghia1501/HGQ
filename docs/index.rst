.. High Granularity Quantization documentation master file, created by
   sphinx-quickstart on Sat Nov 25 16:28:11 2023.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

High Granularity Quantization for Ultra-Fast Inference on FPGAs
===============================================================

HGQ is a method for quantization aware training of neural works to be deployed on FPGAs, which allows for per-weight and per-activation bitwidth optimization.

Depending on the specific [application](https://arxiv.org/abs/2006.10159), HGQ could achieve up to 10x resource reduction compared to the traditional `AutoQkeras` approach, while maintaining the same accuracy. For some more challenging [tasks](https://arxiv.org/abs/2202.04976), where the model is already under-fitted, HGQ could still improve the performance under the same on-board resource consumption. For more details, please refer to our paper (link coming not too soon).

This repository implements HGQ for `tensorflow.keras` models. It is independent of the [QKeras project](https://github.com/google/qkeras).

Notice: this repository is still under development, and the API might change in the future.



Index
=========================================================

.. toctree::
   :maxdepth: 2
   :caption: Contents:

   status.md
   install.md
   getting_started.md
   reference.md
   qkeras.md
   tips.md
   faq.md


.. toctree::
    :hidden:
    :glob:
    :caption: Autogenerated API Reference

    autodoc/HGQ.layers
    autodoc/HGQ.proxy
    autodoc/HGQ.quantizer
    autodoc/HGQ.utils

Indices and tables
==================


* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
